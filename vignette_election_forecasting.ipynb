{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Bayesian Statistics**"
      ],
      "metadata": {
        "id": "QmYVQVS8DeWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two fundamental philosophies when it comes to statistical inference: frequentist and bayesian perspectives. The frequentist approach interprets probability as the long-run frequency of events across repeated trials and treats parameters as fixed but unknown constants. In contrast, the Bayesian approach views parameters as random variables with associated probability distributions. Initial beliefs about the parameters are combined with the observed data to yield posterior distributions. By formally incorporating prior knowledge, such as historical patterns, expert judgment, or previous datasets, the Bayesian approach produces more stable and realistic estimates, especially when data are limited or noisy. Additionally, it is especially well-suited for scenarios involving continuous data monitoring, since bayesian inference can seamlessly update the posterior estimates as new information becomes available.\n",
        "\n",
        "Bayesian inference is grounded in **Bayes' Theorem**, which updates prior beliefs about a parameter using observed data:\n",
        "\n",
        "$$\n",
        "p(\\theta \\mid D) \\propto {p(D \\mid \\theta)\\, p(\\theta)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- **$\\theta$** represents the hypothesis or model parameter.  \n",
        "- **$D$** is the observed data.  \n",
        "- **$p(\\theta)$** is the **prior distribution**, expressing beliefs about $\\theta$ before observing the data.  \n",
        "  These priors may come from past research, expert knowledge, or information about the experimental setup, making external context explicit in the analysis.\n",
        "- **$p(D \\mid \\theta)$** is the **likelihood**, the probability of observing the data given the parameter value $\\theta$.    \n",
        "- **$p(\\theta \\mid D)$** is the **posterior distribution**, the updated belief about $\\theta$ after incorporating both the prior information and the observed data.\n",
        "\n",
        "\n",
        "Bayesian Hierarchical Modeling (BHM) is a statistical technique used when data is naturally structured in levels or groups, such as students by classroom or states within a country. As an advanced technique of hierarchical modeling, each group's parameters are assumed to not be entirely independent. As such, BHM organizes data into multiple levels by assuming that each group's parameter is a product of the higher-level's shared distribution. This higher-level distribution consists of _hyperparameters_, which are simply parameters of the parent (higher-level) distribution. Through BHM, we are able to replicate robust data which can help offset groups with limited data. Known as _partial pooling_, groups with insufficient data are naturally gravitated towards the population mean, while groups with sufficent data have greater independence with respect to their observations. Through this, BHM is able to stabilize estimates and control for noise or other unreliable values."
      ],
      "metadata": {
        "id": "htqznPGcDllW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Application: Predicting Election Results with Pyro**"
      ],
      "metadata": {
        "id": "hTcOwzCevqzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian approaches are a natural fit for election forecasting because they help us integrate what we already know about past elections with whatever fresh polling data we have on hand. By capturing long-term trends and specific state interactions, a Bayesian model can bind everything together rather than considering each state as a separate coin flip. In this situation, we are using Pyro because it allows us to create probabilistic models directly in Python while maintaining access to tools like sampling and inference methods.\n",
        "\n",
        "The concept is straightforward: use previous elections to create a prior assumption about each swing state, then utilize poll data to refine those assumptions, and lastly, run thousands of simulations to determine how frequently Democrats win 270 electoral votes.\n",
        "\n",
        "The model gives an initial probability of around 0.68 that the Democrats will win before any polls are conducted. The 2012 baseline and historical voting trends are reflected in this figure.\n",
        "\n",
        "Using two different forms of polling data, the notebook next generates two posterior predictions:\n",
        "  1. Synthetic Poll (generated from the true 2016 state preferences)\n",
        "\n",
        "  *   A posterior of about 0.58 is produced when the model is given a poll that\n",
        "      reflects the actual underlying 2016 findings. This is a positive indicator. Although the poll only included swing states and contained sampling noise, it demonstrated how the model answered by reducing the likelihood of a Democratic victory while maintaining some uncertainty\n",
        "  2. Scaled Actual 2016 Vote Shares\n",
        "\n",
        "  *   A similar update is obtained when the actual 2016 vote percentages are scaled to the same poll size. In this way, it behaves like a real \"in-cycle\" poll and demonstrates how the model would have changed expectations throughout the course of the campaign.\n",
        "\n"
      ],
      "metadata": {
        "id": "NuGLakFrvpEp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cEbg5I8VqsEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0582a8b3-8ddd-49ab-93ad-a2a544009f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.12/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (2.0.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (3.4.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (0.1.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->pyro-ppl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->pyro-ppl) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# This file includes code adapted from Pyro tutorials\n",
        "# Source: https://pyro.ai/examples/elections.html\n",
        "# Licensed under the Apache License, Version 2.0\n",
        "\n",
        "# Simplified Bayesian election model using swing states only\n",
        "# Outputs TWO posterior results:\n",
        "# 1. Using synthetic poll generated from 2016 results\n",
        "# 2. Using actual 2016 vote percentages scaled to poll size\n",
        "\n",
        "!pip install pyro-ppl\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/pyro-ppl/datasets/master/us_elections/\"\n",
        "\n",
        "# ============================================================\n",
        "# LOAD DATA\n",
        "# ============================================================\n",
        "\n",
        "electoral_college_votes = pd.read_pickle(BASE_URL + \"electoral_college_votes.pickle\")\n",
        "ec_votes_tensor = torch.tensor(electoral_college_votes.values,\n",
        "                               dtype=torch.float).squeeze()\n",
        "\n",
        "frame = pd.read_pickle(BASE_URL + \"us_presidential_election_data_historical.pickle\")\n",
        "\n",
        "# Historical swing states (2000–2020)\n",
        "swing_states = ['FL','PA','MI','WI','OH','NC','GA','NV', 'CO', 'NH']\n",
        "swing_indices = [frame.index.get_loc(st) for st in swing_states]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PRIOR FROM HISTORICAL DATA\n",
        "# ============================================================\n",
        "\n",
        "results_2012 = torch.tensor(frame[2012].values, dtype=torch.float)\n",
        "prior_mean = torch.log(results_2012[..., 0] / results_2012[..., 1])\n",
        "\n",
        "idx = 2 * torch.arange(10)\n",
        "all_results = torch.tensor(frame.values, dtype=torch.float)\n",
        "logits = torch.log(all_results[..., idx] / all_results[..., idx + 1]).transpose(0, 1)\n",
        "\n",
        "mean = logits.mean(0)\n",
        "sample_cov = (1/(logits.shape[0] - 1)) * (\n",
        "    (logits.unsqueeze(-1) - mean) * (logits.unsqueeze(-2) - mean)\n",
        ").sum(0)\n",
        "\n",
        "prior_covariance = sample_cov + 0.01 * torch.eye(sample_cov.shape[0])\n",
        "prior_dist = dist.MultivariateNormal(prior_mean, covariance_matrix=prior_covariance)"
      ],
      "metadata": {
        "id": "VRXF51W99Qhl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block constructs the prior distribution over each state's latent Democratic vs. Republican preference in 2016.\n",
        "\n",
        "Here, we take the Democratic and Republican vote totals from 2012 and convert them into log-odds, which are used as the expected party preference for each state in 2016. 2012 is used as the prior because it is the most recent election before 2016, which in election modeling, the previous result is usually the single best predictor of the next.\n",
        "\n",
        "We then use data from 1976-2012 for covariance, since historical patterns over 40 years reveal more information about the volatility of the state outcomes.\n",
        "\n",
        "Next, we created table to better understand our prior distribution. Our tables aim to learn what our top 3 swing states are, along with the 3 states safest to vote Democrat and 3 safest to vote Republican."
      ],
      "metadata": {
        "id": "hBhxuVHYY1uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log-Odds back to Logit (0-100%)\n",
        "prior_probs = 1 / (1 + torch.exp(-prior_mean))\n",
        "\n",
        "# Standard Deviation from the Covariance Matrix\n",
        "prior_stds = torch.sqrt(torch.diag(prior_covariance))\n",
        "\n",
        "# 3. Create a readable list\n",
        "state_names = frame.index if hasattr(frame, 'index') else [f\"State {i}\" for i in range(len(prior_mean))]\n",
        "\n",
        "data = zip(state_names, prior_probs, prior_stds)\n",
        "\n",
        "# 4. Sort by Swing States\n",
        "sorted_data = sorted(data, key=lambda x: abs(x[1] - 0.5))\n",
        "\n",
        "print(f\"{'Swing States':<15} | {'Predicted Democrat':<18} | {'Uncertainty':<20}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Show top 3 Swing states\n",
        "for name, prob, std in sorted_data[:3]:\n",
        "    print(f\"{name:<15} | {prob.item():.1%} | {std.item():.4f}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Top 3 Democrat States\n",
        "print(\"Safe to Vote Democrat:\")\n",
        "sorted_by_prob = sorted(sorted_data, key=lambda x: x[1], reverse=True)\n",
        "for name, prob, std in sorted_by_prob[:3]:\n",
        "    print(f\"{name:<15} | {prob.item():.1%} | {std.item():.4f}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Top 3 Republican States\n",
        "print(\"Safe to Vote Republican:\")\n",
        "for name, prob, std in sorted_by_prob[-3:]:\n",
        "    print(f\"{name:<15} | {prob.item():.1%} | {std.item():.4f}\")\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrpAgY-VV8jw",
        "outputId": "db6a7756-71cf-4675-df9d-9c31d220516c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Swing States    | Predicted Democrat | Uncertainty         \n",
            "------------------------------------------------------------\n",
            "FL              | 50.4% | 0.2809\n",
            "NC              | 49.0% | 0.2269\n",
            "OH              | 51.5% | 0.1958\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Safe to Vote Democrat:\n",
            "DC              | 92.6% | 0.3712\n",
            "HI              | 71.7% | 0.4054\n",
            "VT              | 68.2% | 0.4271\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "Safe to Vote Republican:\n",
            "OK              | 33.2% | 0.2753\n",
            "WY              | 28.8% | 0.3019\n",
            "UT              | 25.4% | 0.2893\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on our prior distribution, we see that the top 3 swing states are Florida, North Carolina, and Ohio. We also see that Washington D.C, Hawaii, and Vermont are safest to vote democrat, while Oklahoma, Wyoming, and Utah are safest to vote republican.\n",
        "\n",
        "We also add regularization to the covariance to avoid overconfidence in the covariance."
      ],
      "metadata": {
        "id": "4kP1DvwEEWPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# NATIONAL OUTCOME FUNCTION\n",
        "# ============================================================\n",
        "\n",
        "def election_winner(alpha_logits):\n",
        "    dem_win_state = (alpha_logits > 0).float()\n",
        "    dem_votes = ec_votes_tensor * dem_win_state\n",
        "    return (dem_votes.sum() >= 270).float()\n"
      ],
      "metadata": {
        "id": "akagkHujEAqc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function computes the national election outcome given a vector of state-level log-odds."
      ],
      "metadata": {
        "id": "PHjBKnaRI7hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# POSTERIOR INFERENCE VIA IMPORTANCE SAMPLING\n",
        "# ============================================================\n",
        "\n",
        "def posterior_win_prob_given_y(y_obs, allocation, num_alpha_samples=5000):\n",
        "    \"\"\"Approximate P(Dem win | observed poll y_obs).\"\"\"\n",
        "    alpha_samples = prior_dist.sample((num_alpha_samples,))\n",
        "    dem_win = torch.stack([election_winner(a) for a in alpha_samples])\n",
        "\n",
        "    binom = dist.Binomial(total_count=allocation, logits=alpha_samples)\n",
        "    log_lik = binom.log_prob(y_obs).sum(-1)\n",
        "\n",
        "    maxlog = log_lik.max()\n",
        "    weights = torch.exp(log_lik - maxlog)\n",
        "\n",
        "    return ((weights * dem_win).sum() / weights.sum()).clamp(1e-6, 1 - 1e-6)"
      ],
      "metadata": {
        "id": "p1qDRTD6EJNf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function computes the posterior probability of a Democratic win given poll data. It samples state-level preferences from the prior, checks which scenarios result in a Democratic win, and weights each scenario by how likely it is to produce the observed poll results. The weighted average of these outcomes gives the posterior probability, updating our prior belief based on the poll data."
      ],
      "metadata": {
        "id": "R5d97RXDKCnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# PRIOR DEM WIN PROBABILITY (PRINT THIS)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n==============================================\")\n",
        "print(\"Computing PRIOR distribution…\")\n",
        "print(\"==============================================\")\n",
        "\n",
        "alpha_prior_samples = prior_dist.sample((25000,))\n",
        "prior_wins = torch.stack([election_winner(a) for a in alpha_prior_samples])\n",
        "prior_prob = prior_wins.mean().item()\n",
        "\n",
        "print(f\"\\nPrior probability of DEMOCRATIC win (national): {prior_prob:.4f}\")\n",
        "print(\"\\n==============================================\\n\")\n"
      ],
      "metadata": {
        "id": "bXLMi5UcJkgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd4a5b4-d69d-42bb-f74f-51e68c1695a2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================================\n",
            "Computing PRIOR distribution…\n",
            "==============================================\n",
            "\n",
            "Prior probability of DEMOCRATIC win (national): 0.6782\n",
            "\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block computes the prior probability of a Democratic win before seeing any poll data. It samples many scenarios from the prior distribution, checks which ones result in a Democratic victory using the election_winner function, and averages these outcomes to estimate the national win probability based solely on historical data and 2012 results."
      ],
      "metadata": {
        "id": "hkzhdniQKIQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# LOAD TRUE 2016 RESULTS\n",
        "# ============================================================\n",
        "\n",
        "test_data = pd.read_pickle(BASE_URL + \"us_presidential_election_data_test.pickle\")\n",
        "results_2016 = torch.tensor(test_data.values, dtype=torch.float)\n",
        "true_alpha_2016 = torch.log(results_2016[..., 0] / results_2016[..., 1])\n"
      ],
      "metadata": {
        "id": "umiVe0BCJlI8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the actual 2016 election results for each state and convert them into log-odds to represent the true underlying Democratic versus Republican preference. These values are later used to generate synthetic and real polls for comparison with the model's predictions."
      ],
      "metadata": {
        "id": "He7WxI8eKNuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# POLL SIZE AND ALLOCATION\n",
        "# ============================================================\n",
        "\n",
        "TOTAL_POLL = 1500\n",
        "allocation = torch.zeros(51)\n",
        "per_state = TOTAL_POLL // len(swing_states)\n",
        "\n",
        "for st in swing_states:\n",
        "    allocation[frame.index.get_loc(st)] = per_state\n",
        "\n",
        "# Remainder → Florida\n",
        "allocation[frame.index.get_loc('FL')] += TOTAL_POLL - allocation.sum()\n"
      ],
      "metadata": {
        "id": "eNTVq-h4JqLk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block sets up the poll size and how respondents are allocated across states. A total of 1,500 poll respondents is distributed evenly among the swing states, with any remaining respondents added to Florida. This allocation is used to simulate state-level polling data for both synthetic and real 2016 polls."
      ],
      "metadata": {
        "id": "2X1M5w1fKSEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# OPTION 1 — SYNTHETIC POLL GENERATED FROM MODEL\n",
        "# ============================================================\n",
        "\n",
        "# Generating SYNTHETIC poll results based on 2016 true preferences\n",
        "\n",
        "y_synth = torch.zeros(51)\n",
        "\n",
        "for st in swing_states:\n",
        "    idx = frame.index.get_loc(st)\n",
        "    total_polled = allocation[idx]\n",
        "\n",
        "    p_dem = torch.sigmoid(true_alpha_2016[idx])\n",
        "    y_synth[idx] = dist.Binomial(total_count=total_polled, probs=p_dem).sample()\n",
        "\n",
        "# ============================================================\n",
        "# OPTION 2 — ACTUAL 2016 PERCENTAGES AS POLL RESULTS\n",
        "# ============================================================\n",
        "\n",
        "# Generating poll using ACTUAL 2016 vote percentages\n",
        "\n",
        "y_real = torch.zeros(51)\n",
        "\n",
        "for st in swing_states:\n",
        "    idx = frame.index.get_loc(st)\n",
        "    total_polled = allocation[idx].item()\n",
        "\n",
        "    dem_votes = results_2016[idx, 0]\n",
        "    rep_votes = results_2016[idx, 1]\n",
        "    total_votes = dem_votes + rep_votes\n",
        "\n",
        "    p_dem = dem_votes / total_votes\n",
        "    y_real[idx] = (p_dem * total_polled).round()\n"
      ],
      "metadata": {
        "id": "rAFE59h8Jq0T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we generate two types of polls for the swing states. The first, synthetic poll, simulates survey results by sampling from a binomial distribution using the true 2016 state preferences, serving as a sanity check for the model. The second uses the actual 2016 vote percentages scaled to the poll size to create a “realistic” poll, which allows comparison of the model's posterior predictions against the actual election outcomes."
      ],
      "metadata": {
        "id": "buvE12N0KWcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# COMPUTE BOTH POSTERIORS\n",
        "# ============================================================\n",
        "\n",
        "posterior_synth = posterior_win_prob_given_y(y_synth, allocation)\n",
        "posterior_real = posterior_win_prob_given_y(y_real, allocation)"
      ],
      "metadata": {
        "id": "zmoVnXQfJzbN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we compute the posterior probability of a Democratic win for both the synthetic and actual 2016 polls. Using the posterior_win_prob_given_y function, it updates the prior belief based on the observed poll data, producing posterior estimates that reflect how likely Democrats are to win given either the model-generated or real poll results."
      ],
      "metadata": {
        "id": "wZkxUYZJKa4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n==============================================\")\n",
        "print(\"FINAL POSTERIOR RESULTS\")\n",
        "print(\"----------------------------------------------\")\n",
        "print(f\"Posterior probability of a Democratic win (synthetic poll): {posterior_synth.item():.4f}\")\n",
        "print(f\"Posterior probability of a Democratic win (actual 2016 percentages): {posterior_real.item():.4f}\")\n",
        "print(\"==============================================\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFKnvqcDLppF",
        "outputId": "8043c9fe-e9a7-4ce0-ac66-f7767cb79b6a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================================\n",
            "FINAL POSTERIOR RESULTS\n",
            "----------------------------------------------\n",
            "Posterior probability of a Democratic win (synthetic poll): 0.3103\n",
            "Posterior probability of a Democratic win (actual 2016 percentages): 0.4957\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion & Further Reading**"
      ],
      "metadata": {
        "id": "X9_xdQDfyVf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This projects explores a concise yet comprehensive Bayesian election model. The notebook demonstrates how a model may transition from a history-based expectation to a poll-informed prediction by using historical voting data as a multivariate prior, transforming state preferences into log-odds, and updating those beliefs with polling information through importance sampling. Despite only concentrating on swing states and employing a simple binomial likelihood, the model still captures the key trends of the 2016 election: Democrats begin with a strong historical lead, but their prospects decline as poll data that mirror the reality of 2016 are included.\n",
        "\n",
        "The transparency of the entire system is what makes it so helpful. Every assumption is made publicly. You can change the covariance to alter the degree of correlation between states and can even include additional weight for polling errors by baking it in. Due to the underlying components like priors, likelihoods, and sampling being arranged in a structured manner, Pyro makes any future adjustments simple."
      ],
      "metadata": {
        "id": "97X5Hj19yYVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some natural directions we can take this further if we would like to explore:\n",
        "\n"
      ],
      "metadata": {
        "id": "8bOkZKYAzNAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Enhancing via Monte Carlo Methods**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MaEay-Vs0WNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The notebook uses importance sampling, which is simple and effective for minimal models, but once the model becomes more expressive with more states, parameters, turnout uncertainty, it's important that we bolster our important sampling methods.  The weights become unbalanced, and the majority of the samples contribute little to nothing.\n",
        "By sampling directly from the posterior instead of reweighting previous samples, Monte Carlo methods address this issues. Monte Carlo brings improved scaling where it can handle higher-dimensional posteriors without collapsing. This implies that you may use latent variables to account for poll bias, add further layers, or model all 50 states with assurance. MCMC also provide more accurate uncertainty.\n",
        "The whole form of the distribution is represented by posterior samples from MCMC, not only the \"best-fitting\" portion. This provides more accurate predictions of tail scenarios and correlated changes. MC also bring a more realistic simulation result. Every MCMC draw is a self-consistent \"world\" that honors the revised posterior. You get a more complex set of results when you turn those draws into election simulations, which is exactly how forecasting boutiques create their thousands of simulations.\n"
      ],
      "metadata": {
        "id": "5VHovk-0zgyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Introducing Time Series Structure**\n"
      ],
      "metadata": {
        "id": "RTerAtXy0m-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The election is now treated by the model as a single, static update: one prior, one poll, and one posterior. Polls arrive in a sequence, not all at once, and actual campaigns change over time. This slow progression may be captured by the model if a time-series component is added. One popular method is to represent each state's latent log-odds as a random walk or a basic autoregressive process, in which today's preference is yesterday's preference plus a little, normally distributed change. This promotes transitions over sudden jumps and the model, which has a temporal prior in place, can update the posterior each time fresh polling data comes in, creating a trajectory of state preferences leading up to Election Day. The time-series strucuter also helps in distinguishing actual opinion changes from poll noise, as it favors gradual shifts unless the evidence suggests otherwise. In reality, this transforms the model from a single-step into a model that monitors how uncertainty and voter behavior changes over the course of the campaign."
      ],
      "metadata": {
        "id": "jmek8j5o0vIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Enhancing our Hierarchical Model via Partial Pooling**"
      ],
      "metadata": {
        "id": "CwYHKNVEQalH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our current model uses a single, fixed prior to introduce correlation between states. To improve our model, we could implement a Hierarchical Bayesian Model to enable partial pooling, allowing states to borrow strength from national trends when their own data is insufficient. Through this, state-level parameters are treated as possible outcomes from a parent (higher-level) distribution governed by national hyperparameters, which are themselves estimated from the data. This makes the model less fixed and more dynamic, allowing information to be updated at both the state and national levels jointly. Overall, this updated model would provide safer estimates for states with sparse data, creating a more unified protocol for election forecasting."
      ],
      "metadata": {
        "id": "R0vsGLMDQyxD"
      }
    }
  ]
}